{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML02 \n",
    "\n",
    "Expctation & covariance \n",
    "\n",
    "Correlation \n",
    "\n",
    "Gaussian distribution\n",
    "\n",
    "Recap on generalisastion \n",
    "- Estimating generalisation error \n",
    "\n",
    "Bootstrapping \n",
    "\n",
    "Bayesian Framework \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Framework \n",
    "\n",
    "Assumes that a prior distribution always exists. This may be very vague: \n",
    "- when we see some data, we combine our prior distribution with a likelihood function to get a posterior distribution (and keep applying Bayes theorem iteratively). \n",
    "<img src= \"ml4.png\" width =80% img/>\n",
    "<img src= \"ml3.png\" width =80% img/>\n",
    "<img src= \"ml2.png\" width =75% img/>\n",
    "\n",
    "Prior  X Likelihood is proportinal to the Posterior\n",
    "\n",
    "<img src= \"ml1.png\" width =50% img/>\n",
    "\n",
    "\n",
    "#### Density estimation \n",
    "We would like to model the probability distribution p(x) of a random variable x given a finite site D = {x<sub>1</sub>,...,x<sub>N</sub>} of observation. \n",
    "\n",
    "The probalem of density estimation is fundamentally ill-posed because there are infinitely many probability distributions that could have given rise to the finite data set observed. \n",
    "\n",
    "#### Sampling assumption\n",
    "- assume that the training examples are drawn independently from the set of all possible examples. \n",
    "- assume that each time a training example is drawn, it comes from an identical distribution (i.i.d). \n",
    "- assume that the test examples are drawn in exactly the same way: i.i.d and from the same distribution as the training data. \n",
    "- assumptions make it very unlikely that a strong regularity in the training data will be absent in the test data. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "#### Squared error: \n",
    "Squared difference between actual and target real-valued outputs (squared error\n",
    "\n",
    "#### Count of classifcation errors \n",
    "Derivative is not smooth - problem for optimsiation \n",
    "\n",
    "#### Negative log probability \n",
    "Assigned to the correct answer: \n",
    "- in some cases, the same as squared error (e.g. regression with Gaussian output noise); in other cases it is very different. \n",
    "\n",
    "## Maximising log likelihood\n",
    "\n",
    "Suppose that the probability of a coin landing heads (p(x=h)=U) is not necessarily the same as that of it landing tails p(x=t)=1-U\n",
    "\n",
    "We can construct a likelihood function of the assumption that the observations are drawn independently (iid): \n",
    "<img src= \"ML6.png\" width =40% img/>\n",
    "\n",
    "\n",
    "We can estimate a value for U by maximising the log ofthe likelihood function; if we set the derivative of log p (D/U) with respoect to U equal to zero, we obtain the maximum likelihood estimator, where m is the number of heads: \n",
    "<img src= \"ML5.png\" width =30% img/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
