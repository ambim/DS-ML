{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML03:  Bayesian approaches\n",
    "\n",
    "In this notebook: \n",
    "- Naive bayes \n",
    "- Bayesian networks \n",
    "- K2 Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes\n",
    "\n",
    "Applying Bayes Theorem to ML problems - think about the white wine problem. \n",
    "\n",
    "- A popular baseline method for text classification with assumption of independence among variables. \n",
    "- Given x = (x<sub>1</sub>,...,x<sub>n</sub> representing n variables (features), calculating the probability tables is intractable with large n (e.g. words appearing in a document), where k below is the number of document classes/ types:\n",
    "<img src= \"bayes.png\" width =30% img/>\n",
    "- Under maximum-likelihood this can be done by evaluating an expression in linear time, rather than by iterative approximation. ...\n",
    "- Scalable, requiring a number of parametesr linear on the number of variables (e.g. word frequencies). \n",
    "\n",
    "With the Naive conditional independence assumption \n",
    "<img src= \"condind.png\" width =60% img/>\n",
    "\n",
    "Combine Naive Bayes model with a decision rule e.g. *maximum a posteriori* or **MAP decision rule**, which selects the most probable hypothesis. \n",
    "<img src= \"argmax.png\" width =40% img/>\n",
    "\n",
    "**Example:**\n",
    "<img src= \"eg.png\" width =60% img/>\n",
    "\n",
    "**Working:**\n",
    "<img src= \"working.png\" width =60% img/>\n",
    "*note: should be ~runny when calculating posteriors*\n",
    "\n",
    "\n",
    "\n",
    "### Naive Bayes family of classifiers\n",
    "A **class prior** may be calculated by assuming equiprobable classes: prior = 1 / (number of classes), or by calculating an estimate for the class probability from the training set: \n",
    "class prior = (number of samples in the class) / (total number of samples) \n",
    "\n",
    "Variations: gaussian, multinomial, Bernoulli naive, Bayes etc. \n",
    "Despite the naive conditional independence assumption, naive bayes classifiers can be suprisingly efficient on various datasets...\n",
    "\n",
    "\n",
    "**Regularisation** \n",
    "What if just one of many conditional probabilities in:\n",
    "<img src= \"condprob.png\" width =30% img/>\n",
    "... is equal to zero? \n",
    "Use Laplace (a.k.a. \"add one\") smoothing: \n",
    "Let **x** = (x<sub>1</sub>,...,x<sub>n</sub> be observation from a multinomial distribution with N trials (x<sub>i</sub> is the number of times outcome i is observed). A smooted version of each x<sub>i</sub> is given by (x<sub>i</sub>+1)/(N+d). \n",
    "The resulting estimate will be between the empirical probability (relative frequency) x<sub>i</sub>/M and the uniform probability 1/d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian nets\n",
    "\n",
    "In Naive bayes, we assumed no relationship between variables. In bayes nets, we use the K2 algorithm to learn the structure of the network. In a decision tree, there are no cycles, a bayes net is a graph - this gives us a dependency between random variables.  \n",
    "\n",
    "**Use cases**:\n",
    "- influence diagrams (must be acyclic)\n",
    "\n",
    "<img src= \"influence.png\" width =60% img/>\n",
    "\n",
    "**Sequential decision**:\n",
    "The model shows, e.g. that the decision to buy shares can affect the share price. \n",
    "<img src= \"money.png\" width =60% img/>\n",
    "\n",
    "**Uncertainties may influence each other**: \n",
    "We are trying to learn these influences\n",
    "<img src= \"x.png\" width =60% img/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DGMs: Directed Graphical Models conditional independence (1) \n",
    "\n",
    "Consider *a, b, c* such that: \n",
    "\n",
    "*p(a|b,c) = p(a|c)* \n",
    "\n",
    "We say that *a* in **conditionally independent** of *b* given *c*. \n",
    "\n",
    "Conditional independence propoerties of the joint distribution can be read directly from the graph without having to perform any analytical manipulations. \n",
    "\n",
    "#### DGMs: conditional independence (2) \n",
    "<img src= \"dgm2.png\" width =60% img/>\n",
    "\n",
    "\n",
    "#### DGMs: conditional independence (3) \n",
    "<img src= \"dgm3.png\" width =60% img/>\n",
    "\n",
    "#### DGMs: conditional independence (4) \n",
    "<img src= \"dgm4.png\" width =60% img/>\n",
    "\n",
    "#### DGMs: conditional independence (5) \n",
    "<img src= \"dgm5.png\" width =40% img/>\n",
    "\n",
    "#### DGMs: conditional independence (6) \n",
    "<img src= \"dgm6.png\" width =60% img/>\n",
    "\n",
    "#### DGMs: conditional independence (7) \n",
    "<img src= \"dgm7.png\" width =60% img/>\n",
    "\n",
    "#### Structure learning \n",
    "<img src= \"dgm8.png\" width =60% img/>\n",
    "\n",
    "\n",
    "#### Score and search... \n",
    "\n",
    "Start from an initial structure (generated randomly or from domain knowledge) and move to the neighbour with the best score in the structure space until a local maximum of the **score function** is reached. \n",
    "This greedy learning process can re-start several times with the different initial structures to improve the result. \n",
    "\n",
    "\n",
    "#### How many Bayesian nets? \n",
    "Number of DAGs is **super-exponential** on the number of variables. \n",
    "<img src= \"dag.png\" width =60% img/>\n",
    "\n",
    "\n",
    "**Score function:** evaulates how well a given DAG matches the data, e.g. apply maximum likelihood and select the DAG that predicts the data with the highest probability. \n",
    "\n",
    "#### Bayesian information criteria\n",
    "Combines the log likelihood with a penalty for larger DAGs: \n",
    "<img src= \"bic.png\" width =30% img/>\n",
    "Where D = data, G = graph, *d* = number of parameters in the graph, N = number of examples in D. \n",
    "\n",
    "\n",
    "## K2 algorithm \n",
    "\n",
    "1. Applies to discrete variables; x in {0,1,2,...}\n",
    "2. Assumes a maximum number **n** of parents for each node. \n",
    "3. Starts from an initial Bayesian network and moves to add parents incrementally to each node (deterministically or stochastically) until a local maximum is reached (i.e. score and search). \n",
    "\n",
    "\n",
    "#### Greedy search\n",
    "Starts at a specific point (an initial tree, network, etc) in the hypothesis space. Considers all nearest neighbours of the current point, and moves to the neighbour that has the highest score (with a probability in the case of stochastic search). If no neighbours have higher score than the current point (i.e. we have reached a local maximum), the algorithm stops. \n",
    "\n",
    "#### K2 heuristic \n",
    "Assumes a total order on the set of variables such that, e.g. if n=2 and order is x<sub>1</sub>,x<sub>2</sub>, x<sub>3</sub>, x<sub>4</sub> then:\n",
    "- x<sub>1</sub> can't have parents\n",
    "- x<sub>2</sub> may have x<sub>1</sub> as parent\n",
    "- x<sub>3</sub> may have x<sub>1</sub> and x<sub>2</sub> as parents\n",
    "- x<sub>4</sub> may have two of x<sub>1</sub>,x<sub>2</sub>,x<sub>3</sub> as parents\n",
    "\n",
    "#### K2 score function \n",
    "<img src= \"k2.png\" width =30% img/>\n",
    "\n",
    "<p> To compare the networks where node x<sub>1</sub> has sets of parents &pi;<sub>i</sub>; the highest score wins </p>:\n",
    "r<sub>i</sub> is the size of the list of all possible values of x<sub>i</sub>. \n",
    "<p>N<sub>ijk</sub> is the number of times in D that x<sub>i</sub> takes the k<sup>th</sup> value and the parents of x<sub>i</sub> in &pi;<sub>i</sub> take the j<sup>th</sup> instance of the Cartesian product </p>\n",
    "\n",
    "\n",
    "## Akaike Information Criteria\n",
    "\n",
    "Akaike information criterion (AIC): Similar to BIC but uses information loss: \n",
    "- AIC = 2*k* - 2*ln*L\n",
    "\n",
    "L is the maximum value of the likelihood function for a model\n",
    "\n",
    "\n",
    "\n",
    "## Mutual information \n",
    "Some scoring functions are based on the concept of Mutual Information I(X,Y): it measures how much information X and Y share, i.e. how much knowing, i.e. how much knowing one reduces uncertainty about the other. \n",
    "\n",
    "\n",
    "\n",
    "# Worked example  \n",
    "In practice, given two of more graphs, for each graph:  \n",
    "a) Estimate maximum likelihood L = max (ln P(D|G)) from your dataset.\n",
    "b) Calculate the Bayesian Information criteria. Pick the best graph. \n",
    "b) Calculate Akaike information criterion. Pick the best graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
